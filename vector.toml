# ------------------------------------------------------------------------------
# Collect Data from Self-Hosted CRDB Nodes with Vector
# ------------------------------------------------------------------------------
# sources: https://vector.dev/docs/reference/configuration/sources/

# CockroachDB Prometheus metrics
[sources.crdb_ui_console]
type = "prometheus_scrape"
##endpoints = [ "https://localhost:8080/_status/vars" ]
endpoints = [ "http://localhost:8080/_status/vars" ]
scrape_interval_secs = 15
instance_tag = "instance"
endpoint_tag = "endpoint"
# Enable tls.* cert path for secured clusters and enable endponts for https
##tls.crt_file = "certs/node.crt"
##tls.ca_file = "certs/ca.crt"
##tls.key_file = "certs/node.key"

# CockroachDB host metrics
[sources.crdb_node_metrics]
type = "host_metrics"
collectors = [ "cpu","memory","disk","network","filesystem" ]
scrape_interval_secs = 15

# CockroachDB logs: /mnt/disks/crdb-disk is a custom path set by the user when
#  deploying CRDB nodes
[sources.crdb_logs]
type = "file"
ignore_older_secs = 600
include = ["/mnt/disks/crdb-disk/cockroach-data/logs/cockroach.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-health.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-security.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-sql-audit.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-sql-auth.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-sql-exec.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-sql-slow.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-sql-schema.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-pebble.log", "/mnt/disks/crdb-disk/cockroach-data/logs/cockroach-telemetry.log"]


# ------------------------------------------------------------------------------
# Transform Data
# ------------------------------------------------------------------------------
# Transforms: https://vector.dev/docs/reference/configuration/transforms/

# Transform CRDB Prometheus metrics to logs for Splunk
[transforms.crdb_metrics_to_logs_splunk]
type = "metric_to_log"
inputs = [ "crdb_ui_console" ]
host_tag = "hostname"
##condition = '.aggregated_histogram != "*"'


# ------------------------------------------------------------------------------
# Print Data collected to console
# ------------------------------------------------------------------------------

# Preview metrics/logs in the console terminal
[sinks.crdb_terminal]
type = "console"
##inputs = [ "crdb_node_metrics","crdb_metrics_to_logs_splunk","crdb_logs","crdb_ui_console" ]
inputs = [ "crdb_metrics_to_logs_splunk" ]
target = "stdout"

  [sinks.crdb_terminal.encoding]
  codec = "json"


# ------------------------------------------------------------------------------
# Send Data with Sinks
# ------------------------------------------------------------------------------
# Sinks: https://vector.dev/docs/reference/configuration/sinks/

# >>>>> Datadog Logs + Custom Metrics
# Tested. See cloud option at https://app.datadoghq.com/

[sinks.crdb_dd_metrics]
type = "datadog_metrics"
inputs = [ "crdb_ui_console", "crdb_node_metrics" ]
default_api_key = "${DATADOG_API_KEY}"

[sinks.crdb_dd_logs]
type = "datadog_logs"
inputs = [ "crdb_logs"]
default_api_key = "${DATADOG_API_KEY}"
region = "us"
compression = "gzip"
site = "datadoghq.com"
tags = ["source:vector","env:dev","collector:live process"]


# >>>>> New Relic Metrics + Logs
# Untested - Syntax based on Vector documentation. See cloud option at https://login.newrelic.com/
[sinks.my_sink_id]
type = "new_relic"
inputs = [ "crdb_ui_console", "crdb_node_metrics" ]
license_key = "${NEWRELIC_KEY}"
account_id = "xxxx"
region = "us"
compression = "gzip"
api = "events"

[sinks.my_sink_id]
type = "new_relic_logs"
inputs = [ "crdb_logs"]
insert_key = "xxxx"
license_key = "${NEWRELIC_KEY}"
compression = "none"
region = "us"


# >>>>> ElasticSearch Metrics + Logs
# Tested in local docker compose ELK stack. See cloud option at https://cloud.elastic.co/
[sinks.my_sink_id]
type = "elasticsearch"
inputs = [ "crdb_ui_console", "crdb_logs", "crdb_node_metrics" ]
## Local elasticsearch endpoint
endpoint = "http://192.168.86.30:9000"
mode = "bulk"
pipeline = "pipeline-name"
compression = "none"



# >>>>> Splunk Metrics + Logs
# Tested in local docker compose Splunk free license. See cloud option at https://login.splunk.com/

# HEC config: https://docs.splunk.com/Documentation/Splunk/9.0.1/Data/UsetheHTTPEventCollector
# HEC Setup: https://splunk.github.io/docker-splunk/EXAMPLES.html#create-standalone-with-hec

[sinks.metrics_to_splunk_hec]
type = "splunk_hec_metrics"
inputs = [ "crdb_node_metrics" ]
endpoint = "https://192.168.86.30:8088"
host_key = "hostname"
index = "{{ host }}"
source = "{{ file }}"
compression = "gzip"
default_token = "abcd1234"
sourcetype = "{{ sourcetype }}"
tls.verify_certificate = false

[sinks.logs_to_splunk_hec]
type = "splunk_hec_logs"
inputs = [ "crdb_metrics_to_logs_splunk" ]
endpoint = "https://192.168.86.30:8088"
host_key = "hostname"
indexed_fields = [ "field1" ]
compression = "gzip"
##default_token = "abcd1234"
default_token = "${SPLUNK_HEC_TOKEN}"
tls.verify_certificate = false

  [sinks.to_splunk_hec_logs.encoding]
  codec = "json"
